{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Conda Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n cleanenv python=3.10\n",
    "!conda activate cleanenv\n",
    "!pip install fiftyone umap-learn numba numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.5 ultralytics transformers opencv-python torch torchvision huggingface open_clip_torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "from ultralytics import YOLO, SAM\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.core.labels as fol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "yolo_model = YOLO(file_path) # Instantiate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load yolo model for obj detection. Import different models, yolo_mode and bio_models were for experimentation. \n",
    "# Model frop precious cell and this clip_model are ones used\n",
    "\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "bio_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n",
    "bio_model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bio_model = bio_model.to(device)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embeddings(image):\n",
    "    '''\n",
    "    Computes CLIP embeddings for a given image.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized CLIP embeddings.\n",
    "    '''\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "    return outputs / outputs.norm(p=2, dim = 1, keepdim=True)\n",
    "\n",
    "def extract_objects_with_embeddings(yolo_model,frame):\n",
    "    '''\n",
    "     Detects objects in a frame and computes embeddings for each detected object.\n",
    "\n",
    "        Args:\n",
    "            frame (numpy.ndarray): Input frame in BGR format.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing embeddings and bounding boxes.\n",
    "    '''\n",
    "    results = yolo_model(frame)\n",
    "    objects = []\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "        if cropped.shape[0] > 0 and cropped.shape[1] > 0:\n",
    "            # Convert the cropped image to PIL format\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "            emb = get_clip_embeddings(pil_image)\n",
    "            objects.append((emb, (x1, y1, x2, y2)))\n",
    "        return objects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image):\n",
    "    \"\"\"\n",
    "    Detect objects in an image using YOLOv8 from ultralytics.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        embedding: CLIP embedding (not used in this example, but available to integrate if needed).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries with keys 'label' and 'bbox'. The bbox is a tuple (x_min, y_min, x_max, y_max).\n",
    "    \"\"\"\n",
    "    # Load a pre-trained YOLOv8 model (ensure you have ultralytics installed)\n",
    "    model = YOLO(\"yolov8n.pt\")  # or use another weight file/model as needed\n",
    "    # model = SAM(\"sam2.1_b.pt\")\n",
    "    file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "    model = YOLO(file_path) # Instantiate your model\n",
    "    # Run the model on the image; the model accepts BGR images if using cv2 images\n",
    "    results = model(image)\n",
    "    detections = []\n",
    "    # Iterate over each result (usually one result per image)\n",
    "    for result in results:\n",
    "        # result.boxes contains detections in xyxy format and other information\n",
    "        for box in result.boxes:\n",
    "            # Extract bounding box coordinates and convert to integers\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            # Append detection as a dictionary\n",
    "            detections.append({\n",
    "                \"bbox\": (int(x1), int(y1), int(x2), int(y2))\n",
    "            })\n",
    "    print(detections)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embeddings_for_detected_objects(cv2image, yolo_model, clip_model):\n",
    "    \"\"\"\n",
    "    Detects objects in an image and returns CLIP embeddings for each detected object.\n",
    "\n",
    "    Args:\n",
    "        cv2image (np.ndarray): The input image in BGR format (from OpenCV).\n",
    "        yolo_model: Initialized Ultralytics YOLO model.\n",
    "        clip_model: Initialized CLIP image encoder.\n",
    "        clip_preprocess_function: Preprocessing function for the CLIP model.\n",
    "        device (str): Device to run CLIP model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "              'bbox': (x1, y1, x2, y2) coordinates of the detected object.\n",
    "              'class_id': class ID from YOLO.\n",
    "              'confidence': confidence score from YOLO.\n",
    "              'clip_embedding': torch.Tensor containing the CLIP embedding for the object.\n",
    "              Returns an empty list if no objects are detected or an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Detect Objects using YOLO\n",
    "    yolo_results = yolo_model(cv2image, verbose=False) # verbose=False to reduce console output\n",
    "\n",
    "    object_embeddings_data = []\n",
    "\n",
    "    if yolo_results and len(yolo_results) > 0:\n",
    "        # Assuming results for a single image, so take the first element\n",
    "        detections = yolo_results[0]\n",
    "        boxes = detections.boxes # Access the Boxes object\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            xyxy = box.xyxy[0].cpu().numpy().astype(int) # Get (x1, y1, x2, y2)\n",
    "       \n",
    "            x1, y1, x2, y2 = xyxy\n",
    "\n",
    "            # 2. Crop the detected object from the original image\n",
    "            # Ensure coordinates are within image bounds and valid\n",
    "            if x1 >= x2 or y1 >= y2:\n",
    "                # print(f\"Warning: Invalid bounding box coordinates for object {i}: {xyxy}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Clamp coordinates to be within image dimensions to avoid errors during cropping\n",
    "            img_h, img_w = cv2image.shape[:2]\n",
    "            x1_c = max(0, x1)\n",
    "            y1_c = max(0, y1)\n",
    "            x2_c = min(img_w, x2)\n",
    "            y2_c = min(img_h, y2)\n",
    "\n",
    "            if x1_c >= x2_c or y1_c >= y2_c: # If clamped box is invalid\n",
    "                # print(f\"Warning: Clamped bounding box is invalid for object {i}: ({x1_c},{y1_c},{x2_c},{y2_c}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            cropped_object_bgr = cv2image[y1_c:y2_c, x1_c:x2_c]\n",
    "\n",
    "            if cropped_object_bgr.size == 0:\n",
    "                # print(f\"Warning: Cropped object {i} is empty. BBox: {xyxy}. Clamped BBox: ({x1_c},{y1_c},{x2_c},{y2_c}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 3. Preprocess the cropped object for CLIP\n",
    "            #    a. Convert BGR (OpenCV) to RGB\n",
    "            cropped_object_rgb = cv2.cvtColor(cropped_object_bgr, cv2.COLOR_BGR2RGB)\n",
    "            #    b. Convert NumPy array to PIL Image\n",
    "            pil_image = Image.fromarray(cropped_object_rgb)\n",
    "            #    c. Apply CLIP preprocessing\n",
    "            inputs = clip_processor(images=pil_image, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            # 4. Get CLIP embedding for the cropped object\n",
    "            with torch.no_grad():\n",
    "                outputs = clip_model.get_image_features(**inputs)\n",
    "            object_embedding =  outputs / outputs.norm(p=2, dim = 1, keepdim=True)\n",
    "\n",
    "            object_embeddings_data.append({\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'clip_embedding': object_embedding.cpu() # Move to CPU if you plan to store/use it there\n",
    "            })\n",
    "\n",
    "    return object_embeddings_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run script to load initial input image with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "yolo_model = YOLO(file_path) # Instantiate your model\n",
    "im_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/PS2222_20220601T162145Z_FWD_ROV01_IMG_0151.JPG'\n",
    "similar_objects = []\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "frame_idx = 0\n",
    "\n",
    "query_image = Image.open(im_path)\n",
    "query_image = cv2.imread(im_path)\n",
    "query_image = cv2.resize(query_image, (1024, 1024), interpolation=cv2.INTER_CUBIC)\n",
    "query_embedding = get_clip_embeddings(query_image)\n",
    "\n",
    "# Convert the frame to RGB for displaying with matplotlib\n",
    "detections = detect_objects(query_image)\n",
    "output_image = query_image.copy()\n",
    "for i,detection in enumerate(detections):\n",
    "    print(detection)\n",
    "    x_min, y_min, x_max, y_max = detection[\"bbox\"]\n",
    "    # Draw rectangle and label\n",
    "    cv2.rectangle(output_image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "    label = str(i)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.6\n",
    "    thickness = 2\n",
    "    text_color = (0, 255, 0)  # Green, matches the box\n",
    "\n",
    "    # Calculate text size (optional, helps align better)\n",
    "    (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "\n",
    "    # Offset so the text is just above the box\n",
    "    text_x = x_min\n",
    "    text_y = y_min - 5 if y_min - 5 > text_height else y_min + text_height + 5\n",
    "\n",
    "    # Put text on image\n",
    "    cv2.putText(output_image, label, (text_x, text_y), font, font_scale, text_color, thickness)\n",
    "    \n",
    "frame_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the frame with the bounding box\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Initial Input Image with Detections\")\n",
    "plt.show()\n",
    "object_embeddings = get_clip_embeddings_for_detected_objects(query_image, yolo_model, clip_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through directory to run object detection and similarity on other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['media_id', 'frame_num', 'bbox', 'embedding_vec']\n",
    "media_id =  '4291234'\n",
    "\n",
    "# Create an empty DataFrame\n",
    "embedding_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "yolo_model = YOLO(file_path) # Instantiate your model\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.6\n",
    "query_embedding = object_embeddings[4]['clip_embedding']\n",
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "\n",
    "# Get all .jpg files in the folder\n",
    "jpg_files = glob.glob(os.path.join(folder_path, '*.JPG'))\n",
    "print(jpg_files)\n",
    "for l,image_path in enumerate(jpg_files):\n",
    "        print(l)      \n",
    "    # if l < 20:\n",
    "        print(f\"Processing: {image_path}\")\n",
    "    \n",
    "\n",
    "    # cap = cv2.VideoCapture(video_path)\n",
    "        query_image = cv2.imread(image_path)\n",
    "        media_id = os.path.basename(image_path)\n",
    "        print(media_id)\n",
    "        objects = get_clip_embeddings_for_detected_objects(query_image, yolo_model, clip_model)\n",
    "        if objects is None:\n",
    "            print(\"No objects detected.\")\n",
    "            continue\n",
    "        for detection in objects:\n",
    "            emb, bbox = detection['clip_embedding'], detection['bbox']\n",
    "            new_row = {\n",
    "                    'media_id': media_id,\n",
    "                    'frame_num': 'image',\n",
    "                    'bbox': bbox,\n",
    "                    'embedding_vec': emb\n",
    "            }\n",
    "            embedding_df.loc[len(embedding_df)] = new_row\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(query_embedding.numpy(), emb.numpy())[0][0]\n",
    "\n",
    "            if similarity > SIMILARITY_THRESHOLD:\n",
    "                similar_objects.append((media_id, bbox, similarity))\n",
    "                print(f'Object added with similarity: {similarity}')\n",
    "                #Optional to draw box\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                cv2.rectangle(query_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(query_image, f\"Similarity: {similarity:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fiftyone demo and embedding df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(embedding_df)\n",
    "image_dir = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "if \"df-object-embeddings\" in fo.list_datasets():\n",
    "    fo.delete_dataset(\"df-object-embeddings\")\n",
    "\n",
    "dataset = fo.Dataset(\"df-object-embeddings\")\n",
    "# dataset = fo.utils.dataset.import_dataset(\n",
    "#     dataset_type=fo.types.ImageDirectory,\n",
    "#     dataset_dir=image_dir,\n",
    "#     name='MDBC_images'\n",
    "# )\n",
    "\n",
    "\n",
    "df = embedding_df.copy()\n",
    "for idx, row in df.iterrows():\n",
    "    image_path = os.path.join(image_dir, row[\"media_id\"])\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "\n",
    "    # Get original image to compute bbox thumbnail\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    x1, y1, x2, y2 = row[\"bbox\"]\n",
    "\n",
    "    # Convert to relative [x, y, w, h]\n",
    "    x = x1 / w\n",
    "    y = y1 / h\n",
    "    box_w = (x2 - x1) / w\n",
    "    box_h = (y2 - y1) / h\n",
    "\n",
    "    # Create a new sample for this single detection\n",
    "    sample = fo.Sample(filepath=image_path)\n",
    "\n",
    "    detection = fol.Detection(\n",
    "        label=\"object\",\n",
    "        bounding_box=[x, y, box_w, box_h],\n",
    "        embedding=np.array(row[\"embedding_vec\"]).squeeze().tolist()\n",
    "    )\n",
    "\n",
    "    sample[\"detections\"] = fol.Detections(detections=[detection])\n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    patches_field=\"detections\",\n",
    "    embeddings=\"embedding\",            # field in each detection\n",
    "    brain_key=\"clip_embedding\",\n",
    "    method=\"umap\",\n",
    "    thumbnails=True\n",
    ")\n",
    "embeddings_panel = fo.Panel(\n",
    "    type=\"Embeddings\",\n",
    "    state=dict(brainResult=\"img_viz\", colorByField=\"uniqueness\"),\n",
    ")\n",
    "# fob.compute_visualization(\n",
    "#     dataset, brain_key=\"gt_viz\", \n",
    "# )\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print photos above threshold for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "output_dir = 'images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for i, (media_id, bbox, sim) in enumerate(similar_objects):\n",
    "    print(f\"Match {i + 1} - Frame: {media_id}, BBox: {bbox}, Similarity: {sim:.2f}\")\n",
    "    if sim > .92:\n",
    "    # if sim > .78 and media_id == 'PS2222_20220601T162145Z_FWD_ROV01_IMG_0151.JPG':\n",
    "        # Optionally save or display the frame with bounding box\n",
    "        # cv2.imwrite(f\"output_frame_{frame_num}.jpg\", frame)\n",
    "        # Open the video and seek to the specific frame\n",
    "        image_path = os.path.join(folder_path, media_id)\n",
    "        frame = cv2.imread(image_path)\n",
    "\n",
    "        # Extract the bounding box\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Convert the frame to RGB for displaying with matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the frame with the bounding box\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {media_id} with Bounding Box\")\n",
    "\n",
    "        # Crop the region corresponding to the bounding box\n",
    "        cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "        cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the cropped region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cropped_region_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Cropped Region (Similarity: {sim:.2f})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the frame with the bounding box to the \"images\" directory\n",
    "        output_file = os.path.join(output_dir, f\"frame_{media_id}_match_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_file, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "output_dir = 'images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for i, (media_id, bbox, sim) in enumerate(similar_objects):\n",
    "    print(f\"Match {i + 1} - Frame: {media_id}, BBox: {bbox}, Similarity: {sim:.2f}\")\n",
    "    if .65 < sim < .67:\n",
    "    # if sim > .78 and media_id == 'PS2222_20220601T162145Z_FWD_ROV01_IMG_0151.JPG':\n",
    "        # Optionally save or display the frame with bounding box\n",
    "        # cv2.imwrite(f\"output_frame_{frame_num}.jpg\", frame)\n",
    "        # Open the video and seek to the specific frame\n",
    "        image_path = os.path.join(folder_path, media_id)\n",
    "        frame = cv2.imread(image_path)\n",
    "\n",
    "        # Extract the bounding box\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Convert the frame to RGB for displaying with matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the frame with the bounding box\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {media_id} with Bounding Box\")\n",
    "\n",
    "        # Crop the region corresponding to the bounding box\n",
    "        cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "        cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the cropped region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cropped_region_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Cropped Region (Similarity: {sim:.2f})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the frame with the bounding box to the \"images\" directory\n",
    "        output_file = os.path.join(output_dir, f\"frame_{media_id}_match_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_file, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "output_dir = 'images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for i, (media_id, bbox, sim) in enumerate(similar_objects):\n",
    "    print(f\"Match {i + 1} - Frame: {media_id}, BBox: {bbox}, Similarity: {sim:.2f}\")\n",
    "    if .84 < sim < .86:\n",
    "    # if sim > .78 and media_id == 'PS2222_20220601T162145Z_FWD_ROV01_IMG_0151.JPG':\n",
    "        # Optionally save or display the frame with bounding box\n",
    "        # cv2.imwrite(f\"output_frame_{frame_num}.jpg\", frame)\n",
    "        # Open the video and seek to the specific frame\n",
    "        image_path = os.path.join(folder_path, media_id)\n",
    "        frame = cv2.imread(image_path)\n",
    "\n",
    "        # Extract the bounding box\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Convert the frame to RGB for displaying with matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the frame with the bounding box\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {media_id} with Bounding Box\")\n",
    "\n",
    "        # Crop the region corresponding to the bounding box\n",
    "        cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "        cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the cropped region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cropped_region_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Cropped Region (Similarity: {sim:.2f})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the frame with the bounding box to the \"images\" directory\n",
    "        output_file = os.path.join(output_dir, f\"frame_{media_id}_match_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_file, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional, for video, run on specific frame to get initial frame for comparison, i.e. input. Can leave as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "yolo_model = YOLO(file_path) # Instantiate your model\n",
    "video_path = '/tmp/PS2222_20220601T175323Z_FWD_ROV01_HD.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "similar_objects = []\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "frame_idx = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    # print(f'reading frame {frame_idx}')\n",
    "    ret, frame = cap.read()\n",
    "    if frame_idx == 2700:\n",
    "        # Save the first frame as a temporary image file\n",
    "        temp_image_path = \"first_frame.jpg\"\n",
    "        cv2.imwrite(temp_image_path, frame)\n",
    "\n",
    "\n",
    "        query_image = Image.open(temp_image_path)\n",
    "        query_image = cv2.imread(temp_image_path)\n",
    "        query_image = cv2.resize(query_image, (1024, 1024), interpolation=cv2.INTER_CUBIC)\n",
    "        query_embedding = get_clip_embeddings(query_image)\n",
    "\n",
    "        # Convert the frame to RGB for displaying with matplotlib\n",
    "        detections = detect_objects(query_image)\n",
    "        output_image = query_image.copy()\n",
    "        for i,detection in enumerate(detections):\n",
    "            if i == 3:\n",
    "                print(detection)\n",
    "                x_min, y_min, x_max, y_max = detection[\"bbox\"]\n",
    "                # Draw rectangle and label\n",
    "                cv2.rectangle(output_image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "        frame_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the frame with the bounding box\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"First Frame\")\n",
    "        plt.show()\n",
    "        object_embeddings = get_clip_embeddings_for_detected_objects(query_image, yolo_model, clip_model)\n",
    "    elif frame_idx >= 3000:\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through video and find similar objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['media_id', 'frame_num', 'bbox', 'embedding_vec']\n",
    "media_id =  '4291234'\n",
    "\n",
    "# # Create an empty DataFrame\n",
    "embedding_df = pd.DataFrame(columns=columns)\n",
    "video_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/PS2222_20220601T175323Z_FWD_ROV01_HD.mp4'\n",
    "file_path = \"./Benthic-Mapping-highlight/benthic_mapping/model.pt\"\n",
    "yolo_model = YOLO(file_path) # Instantiate your model\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "similar_objects = []\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "frame_idx = 0\n",
    "# query_embedding = object_embeddings[0]['clip_embedding']\n",
    "while cap.isOpened():\n",
    "    print(f'reading frame {frame_idx}')\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "    if frame_idx % 50 == 0 and frame_idx <= 5000:\n",
    "        objects = get_clip_embeddings_for_detected_objects(frame, yolo_model, clip_model)\n",
    "        if objects is None:\n",
    "            print(\"No objects detected.\")\n",
    "            continue\n",
    "        for detection in objects:\n",
    "            emb, bbox = detection['clip_embedding'], detection['bbox']\n",
    "            new_row = {\n",
    "                'media_id': media_id,\n",
    "                'frame_num': frame_idx,\n",
    "                'bbox': bbox,\n",
    "                'embedding_vec': emb\n",
    "            }\n",
    "            embedding_df.loc[len(embedding_df)] = new_row\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(query_embedding.numpy(), emb.numpy())[0][0]\n",
    "\n",
    "            if similarity > SIMILARITY_THRESHOLD:\n",
    "                similar_objects.append((frame_idx, bbox, similarity))\n",
    "                print(f'Object added with similarity: {similarity}')\n",
    "                #Optional to draw box\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Similarity: {similarity:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output similar objects/frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = 'images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for i, (frame_num, bbox, sim) in enumerate(similar_objects):\n",
    "    print(f\"Match {i + 1} - Frame: {frame_num}, BBox: {bbox}, Similarity: {sim:.2f}\")\n",
    "\n",
    "    if sim > .81:\n",
    "        # Optionally save or display the frame with bounding box\n",
    "        # cv2.imwrite(f\"output_frame_{frame_num}.jpg\", frame)\n",
    "        # Open the video and seek to the specific frame\n",
    "        video_capture = cv2.VideoCapture(video_path)\n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        success, frame = video_capture.read()\n",
    "        video_capture.release()\n",
    "\n",
    "        if not success:\n",
    "            print(f\"Unable to read frame {frame_num} from video.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the bounding box\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Convert the frame to RGB for displaying with matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the frame with the bounding box\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {frame_num} with Bounding Box\")\n",
    "\n",
    "        # Crop the region corresponding to the bounding box\n",
    "        cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "        cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the cropped region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cropped_region_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Cropped Region (Similarity: {sim:.2f})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the frame with the bounding box to the \"images\" directory\n",
    "        output_file = os.path.join(output_dir, f\"frame_{frame_num}_match_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_file, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Dataset Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def display_scrollable_df(df, max_height=300, max_width=1000):\n",
    "    display(HTML(df.to_html(notebook=True, max_rows=1000, max_cols=1000,\n",
    "                            border=0, classes='scroll-table')))\n",
    "    styles = f\"\"\"\n",
    "    <style>\n",
    "    .scroll-table {{\n",
    "        display: block;\n",
    "        overflow: auto;\n",
    "        max-height: {max_height}px;\n",
    "        max-width: {max_width}px;\n",
    "        border: 1px solid #ccc;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(styles))\n",
    "display_scrollable_df(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities\n",
    "embedding_df['similarity'] = embedding_df['embedding_vec'].apply(lambda x: cosine_similarity(x, query_embedding)[0][0])\n",
    "\n",
    "# Filter based on a similarity threshold\n",
    "threshold = 0.94\n",
    "filtered_df = embedding_df[embedding_df['similarity'] > threshold]\n",
    "\n",
    "display_scrollable_df(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "media_id = filtered_df['media_id'].iloc[0]\n",
    "image_path = os.path.join(folder_path, media_id)\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# Extract the bounding box\n",
    "bbox = filtered_df['bbox'].iloc[0]\n",
    "x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "# Draw the bounding box on the frame\n",
    "cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# Convert the frame to RGB for displaying with matplotlib\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the frame with the bounding box\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Frame with Bounding Box\")\n",
    "\n",
    "# Crop the region corresponding to the bounding box\n",
    "cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the cropped region\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cropped_region_rgb)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"orange coral\"\n",
    "\n",
    "inputs = clip_processor(text=[text_query], return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs)\n",
    "    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matrix = np.stack(embedding_df[\"embedding_vec\"].values)\n",
    "text_vector = text_features.cpu().numpy()\n",
    "print(text_vector.shape)\n",
    "print(image_matrix.shape)\n",
    "# Make sure text_vector is (1, 512)\n",
    "text_vector = text_vector.squeeze()  # from (1, 1, 512) → (512,)\n",
    "if text_vector.ndim == 1:\n",
    "    text_vector = text_vector[np.newaxis, :]  # reshape to (1, 512)\n",
    "\n",
    "# Make sure image_matrix is (N, 512)\n",
    "if image_matrix.ndim == 3:\n",
    "    image_matrix = image_matrix.squeeze()  # from (N, 1, 512) → (N, 512)\n",
    "# Compute similarity\n",
    "similarities = cosine_similarity(text_vector, image_matrix)[0]\n",
    "embedding_df[\"text_similarity\"] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "top_results = embedding_df.sort_values(\"text_similarity\", ascending=False).head(top_k)\n",
    "print(top_results[[\"media_id\", \"text_similarity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './Benthic-Mapping-highlight/benthic_mapping/data/Images/'\n",
    "media_id = embedding_df['media_id'].iloc[948]\n",
    "image_path = os.path.join(folder_path, media_id)\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# Extract the bounding box\n",
    "bbox = embedding_df['bbox'].iloc[948]\n",
    "x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "# Draw the bounding box on the frame\n",
    "cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# Convert the frame to RGB for displaying with matplotlib\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the frame with the bounding box\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Frame with Bounding Box\")\n",
    "\n",
    "# Crop the region corresponding to the bounding box\n",
    "cropped_region = frame[y_min:y_max, x_min:x_max]\n",
    "cropped_region_rgb = cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the cropped region\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cropped_region_rgb)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
